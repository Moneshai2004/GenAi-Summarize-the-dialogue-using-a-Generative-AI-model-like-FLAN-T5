{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9DEE1q5V0sWt/X2EeTxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moneshai2004/GenAi-Summarize-the-dialogue-using-a-Generative-AI-model-like-FLAN-T5/blob/main/Summarize_the_dialogue_using_a_Generative_AI_model_like_FLAN_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ORKJ4Ki0XeWc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"knkarthick/dialogsum\")\n"
      ],
      "metadata": {
        "id": "UfC6VBXpZNuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "oLz7MLdXbw5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Tokenize Input Dialogue & Generate Summary (Zero-Shot)"
      ],
      "metadata": {
        "id": "432sCVVPchKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dialogue\n",
        "dialogue = \"\"\"\n",
        "Person1: What time is it, Tom?\n",
        "Person2: Just a minute, it's 10 to 9 by my watch.\n",
        "Person1: Are you sure? We need to catch the train.\n",
        "Person2: Don’t worry, we have plenty of time.\n",
        "\"\"\"\n",
        "\n",
        "# Add instruction for the model\n",
        "prompt = \"Summarize the following conversation:\\n\" + dialogue + \"\\nSummary:\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate summary\n",
        "outputs = model.generate(**inputs, max_length=60)\n",
        "\n",
        "# Decode the output tokens to text\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "leyiq0Qkcnqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input with prompt\n",
        "dialogue = \"Person 1: What time is it, Tom?\\nPerson 2: It's 9:50.\\n\"\n",
        "\n",
        "input_text = \"Summarize the following conversation:\\n\" + dialogue + \"\\nSummary:\"\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    num_beams=2,      # use beam search for better quality\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "T5XPyydHerND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-shot example dialogue + summary (the shot)"
      ],
      "metadata": {
        "id": "5qmQbYA1ftha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# One-shot example dialogue + summary (the shot)\n",
        "example_dialogue = \"\"\"\n",
        "Person 1: What time is it, Tom?\n",
        "Person 2: It's 9:50 by my watch.\n",
        "\"\"\"\n",
        "example_summary = \"Person 2 tells Person 1 that the time is 9:50.\"\n",
        "\n",
        "# New dialogue to summarize\n",
        "new_dialogue = \"\"\"\n",
        "Person 1: Is the train leaving soon?\n",
        "Person 2: Yes, it's about to leave in 5 minutes.\n",
        "\"\"\"\n",
        "\n",
        "# Create prompt with one-shot example and new dialogue\n",
        "prompt = f\"Summarize the following conversations.\\n\\nDialogue:\\n{example_dialogue}\\nSummary:\\n{example_summary}\\n\\nDialogue:\\n{new_dialogue}\\nSummary:\"\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate summary\n",
        "outputs = model.generate(**inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "7fMH8jyJfxnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lW9swk8OhNCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " few-shot prompting example in Python with FLAN-T5:\n",
        "python"
      ],
      "metadata": {
        "id": "K2pOT5NhhPqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "few_shot_prompt = \"\"\"\n",
        "Dialogue:\n",
        "Person 1: What time is it?\n",
        "Person 2: It's 9:50.\n",
        "\n",
        "Summary:\n",
        "The time is 9:50.\n",
        "\n",
        "Dialogue:\n",
        "Person 1: I need to upgrade my computer.\n",
        "Person 2: You should add a CD-ROM.\n",
        "\n",
        "Summary:\n",
        "Person 1 wants to upgrade their computer by adding a CD-ROM.\n",
        "\n",
        "Dialogue:\n",
        "Person 1: Tom, are we going to miss the train?\n",
        "Person 2: No, we still have 5 minutes.\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=60)\n",
        "\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "p_zmKQqhhTx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GenerationConfig\n",
        "\n",
        "# Load summarization pipeline (or model + tokenizer separately)\n",
        "summarizer = pipeline(\"summarization\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Generation config example\n",
        "gen_config = GenerationConfig(\n",
        "    max_length=50,   # Limit summary length\n",
        "    temperature=0.7, # Moderate creativity\n",
        "    top_p=0.9,       # Nucleus sampling threshold\n",
        "    top_k=50         # Limits next token candidates\n",
        ")\n",
        "\n",
        "# Input dialogue text\n",
        "few_shot_prompt = \"\"\"\n",
        "Dialogue:\n",
        "Person 1: What time is it?\n",
        "Person 2: It's 9:50.\n",
        "\n",
        "Summary:\n",
        "The time is 9:50.\n",
        "\n",
        "Dialogue:\n",
        "Person 1: I need to upgrade my computer.\n",
        "Person 2: You should add a CD-ROM.\n",
        "\n",
        "Summary:\n",
        "Person 1 wants to upgrade their computer by adding a CD-ROM.\n",
        "\n",
        "Dialogue:\n",
        "Person 1: Tom, are we going to miss the train?\n",
        "Person 2: No, we still have 5 minutes.\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Generate summary with config\n",
        "summary = summarizer(few_shot_prompt, generation_config=gen_config)\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "uUMewWi6ijSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ve learned about:\n",
        "\n",
        "Loading datasets\n",
        "\n",
        "Using pretrained models like FLAN-T5\n",
        "\n",
        "Tokenization basics\n",
        "\n",
        "Zero-shot, one-shot, and few-shot prompting\n",
        "\n",
        "Prompt engineering basics\n",
        "\n",
        "Adjusting generation parameters like temperature\n"
      ],
      "metadata": {
        "id": "XjULEtFWjDtN"
      }
    }
  ]
}